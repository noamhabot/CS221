---
title: "model"
author: "Noam Habot"
date: "12/9/2017"
output: pdf_document
---

Here we load all the files:
```{r}
rm(list = ls())

library(data.table)
library(caTools)
library(zoo)

## Read in the big data set however necessary. 
#data = data.table(read.csv("~/Downloads/dataset.csv"), sep = ',', header = FALSE, fill = TRUE)
#saveRDS(data, "~/Documents/cs221/CS221FinalProject/data.rds")
data = readRDS("~/Google Drive/Stanford/Stanford Y2 Q1/CS 221/CS221FinalProject/data.rds")
data_ = subset(data, select=c(2,4,5,6,7,8,9,10,11,12:ncol(data)))
colnames(data_)

# 2/3 of observations dont have a region 2
data_$region_2 = NULL

# 1/3 of observations dont have a designation
data_$designation = NULL

# Basically one winery per row... not predictive
data_$winery = NULL

# Lots of blanks, remove rows with blank cells
data_[data_==""] = NA
data_s = na.omit(data_)

# Grab the most frequent categories
countries = tail(names(sort(table(data_s$country))), 15)
provinces = tail(names(sort(table(data_s$province))), 50) 
regions = tail(names(sort(table(data_s$region_1))), 50) 
varieties = tail(names(sort(table(data_s$variety))), 100)
# wineries = tail(names(sort(table(data_s$winery))), 1500)

data_s = data_s[country %in% countries & province %in% provinces & region_1 %in% regions & variety %in% varieties,]

# sample = sample.int(n = nrow(data_s), size = floor(.8*nrow(data_s)), replace = F)
# train = data[sample, ]
# test  = data[-sample, ]

set.seed(1)
split = sample.split(data_s$flavors, SplitRatio = 0.8)
train = subset(data_s, split == TRUE)
test = subset(data_s, split == FALSE)
```



```{r}
###### METRICS NEW ########
###########################
prediction <- readRDS("~/Google Drive/Stanford/Stanford Y2 Q1/CS 221/CS221FinalProject/Predictions/predictions_totalfirst1000.rds")


n = 100#nrow(prediction)
randomSample <- sample(1:nrow(prediction), n)
m = ncol(prediction)
added_probabilities <- c(seq(-0.5, 0.39, 0.1), seq(0.4, 0.495, 0.01), seq(0.495, 0.5, 0.001))
Fscoredata <- data.frame(row.names=added_probabilities)
FPRdata <- data.frame(row.names=added_probabilities)
Recalldata <- data.frame(row.names=added_probabilities)

for (endIndex in c(seq(11,91,10), seq(101,351,50))) { # 951
  print(paste("endIndex:", endIndex))
  startIndex <- 1 # by default, this is 7
  #endIndex <- 1001 # by default, this is 1001
  
  recallMeans <- c()
  specificityMeans <- c()
  Fscores <- c()
  
  for (added_probability in added_probabilities) {
    print(added_probability)
  
    recall = rep(0, n)
    precision = rep(0, n)
    specificity = rep(0, n)
    for (i in 1:n) {
      
      thisObsKeywords = test[randomSample[i], (startIndex+6):(endIndex+6)]
      predictions <- round(prediction[randomSample[i], startIndex:endIndex]+added_probability)
      acc = table(Predictions = as.numeric(predictions), TrueLabels = as.numeric(thisObsKeywords))
      TN = acc[1]
      FP = acc[2]
      FN = acc[3]
      TP = acc[4]
      
      if (nrow(acc) == 1) {
        if (rownames(acc)[1] == "1") {
          TN = 0
          FP = acc[1]
          FN = 0
          TP = acc[2]
        } else if (rownames(acc)[1] == "0") {
          TN = acc[1]
          FP = 0
          FN = acc[2]
          TP = 0
        }
  
      }
      if (is.na(TP)) {
        precision[i] = 0
        recall[i] = 0
      } else {
        if (TP == 0) {
          precision[i] = 0
        } else {
          precision[i] = TP / (TP + FP)
        }
        recall[i] = TP / (TP + FN)
      }
      
      specificity[i] = TN/(TN+FP)
    }
    
    recall[is.nan(recall)]=NA
    precision[is.nan(precision)]=NA
    specificity[is.nan(specificity)]=NA
    
    recallScore = mean(recall, na.rm = TRUE)
    precisionScore = mean(precision, na.rm=TRUE)
    specificityScore = mean(specificity, na.rm=TRUE)
    
    Fscore = (2*precisionScore*recallScore)/(precisionScore+recallScore)
    Fscores <- c(Fscores, Fscore)
    
    print(paste("Mean of recall:", recallScore))
    print(paste("Mean of precision:", precisionScore))
    print(paste("Mean of specificity:", specificityScore))
  
    recallMeans <- c(recallMeans, recallScore)
    specificityMeans <- c(specificityMeans, specificityScore)
  }
  Fscoredata <- cbind(Fscoredata, NA)
  names(Fscoredata)[length(Fscoredata)] <- paste("start",startIndex, "end",endIndex)
  Fscoredata[,length(Fscoredata)] <- Fscores
  
  FPR <- (1-specificityMeans)
  FPRdata <- cbind(FPRdata, NA)
  names(FPRdata)[length(FPRdata)] <- paste("start",startIndex, "end",endIndex)
  FPRdata[,length(FPRdata)] <- FPR
  
  Recalldata <- cbind(Recalldata, NA)
  names(Recalldata)[length(Recalldata)] <- paste(startIndex, "-", endIndex,sep="")
  Recalldata[,length(Recalldata)] <- recallMeans
  
  
}

View(Fscoredata)
View(FPRdata)
View(Recalldata)

FscoredataCopy <- Fscoredata
FPRdataCopy <- FPRdata
RecalldataCopy <- Recalldata

max(Fscoredata[2:nrow(Fscoredata), 1:ncol(Fscoredata)-1])
# max occurs when using all 1000 keywords with added_p = 0.3


max(Fscoredata[3:nrow(Fscoredata),1:7])

# generate latex out of these tables
xtable(Fscoredata[,1:7], digits=4)
xtable(Fscoredata[,8:15], digits=4)

############################################################################################################
######### Fscoredata, FPRdata, and Recalldata for keywords with ranges with increasing START indicies 
######### are stored in Predictions/Fscores1.Rdata
######### Fscoredata, FPRdata, and Recalldata for keywords with ranges with increasing START indicies 
######### are stored in Predictions/Fscores2.RData
############################################################################################################

#save(Fscoredata, FPRdata, Recalldata, file="~/Google Drive/Stanford/Stanford Y2 Q1/CS 221/CS221FinalProject/Predictions/Fscores2.RData")
load(file="~/Google Drive/Stanford/Stanford Y2 Q1/CS 221/CS221FinalProject/Predictions/Fscores2.RData")

plotme <- data.frame(x=FPRdata[,"start 1 end 101"], y=Recalldata[,"1-101"], col=c(rep("black", 8), "red", rep("black", nrow(Fscoredata)-8-1)))
row.names(plotme) <- row.names(Fscoredata)






ggplot(data=plotme) + geom_point(aes(x=x, y=y, col=col), size=3)+ geom_line(aes(x=x, y=y)) + 
  labs(title = "ROC Curve for Keyword Range 1-100", x = "False Positive Rate", y = "Recall", color = "Probability Cutoffs") + #,
       #caption="The Optimal probability cutoff calculated in this plot is 0.3.") +
 scale_color_manual(labels = c("Other probability", "Optimal probability"), values = c("lightblue", "red")) +
  theme_bw()

# AREA UNDER THE CURVE: 0.7003093
xt <- diff(plotme$x[plotme$x<1])
yt <- rollmean(plotme$y[plotme$x<1],2)
# This gives you the area
sum(xt*yt)
```










```{r}
##### RUNNING KEYWORDS


#range = 108:357
#range = 358:507
#range = 508:1007
prediction = as.data.table(matrix(nrow = nrow(test), ncol = length(range)))
colnames(prediction) = colnames(test)[range]
for (i in range) {
  print(i)
  thisVariable <- names(train)[i]
  model = glm(as.matrix(train[, get(thisVariable)]) ~ points+price+country+province+region_1+variety, data = train, family = binomial(logit))
  prediction[, thisVariable] = predict(model, newdata = test, type = "response")
}
#saveRDS(prediction, "~/Google Drive/Stanford/Stanford Y2 Q1/CS 221/CS221FinalProject/predictions_108_357.rds")
```









Do in parallel:
```{r}
library(doMC)
registerDoMC()
options(cores=7)
r <- foreach(i=508:1007, .combine=list) %dopar% {
  thisVariable <- names(train)[i]
  model = glm (as.matrix(train[, get(thisVariable)]) ~ points+price+country+province+region_1+variety, data = train, family = binomial(logit))
  prediction = predict(model, newdata = test, type = "response")
  #err = 1 - sum((prediction > 0.5) == (test[, get(thisVariable)] > 0.5))/nrow(test)
  l = list(prediction)
  names(l)[1] = thisVariable
  l
}
#saveRDS(r, "~/Google Drive/Stanford/Stanford Y2 Q1/CS 221/CS221FinalProject/predictions_2.rds")
r = readRDS("~/Google Drive/Stanford/Stanford Y2 Q1/CS 221/CS221FinalProject/predictions_508_1007.rds")


# convert "r" to the nice format of the data
newDF <- data.frame(rep(1,nrow(test)))
# first one
first <- paste("r", paste(rep("[[1]]",499), collapse=""), "", sep="")
newDF[,names(eval(parse(text = first)))] <- eval(parse(text = first))[[1]]
# from 498 to 1
for (i in 498:1) {
  varText <- paste(rep("[[1]]",i), collapse="")
  varText <- paste("r", varText, "[[2]]", sep="")
  newDF[,names(eval(parse(text = varText)))] <- eval(parse(text = varText))[[1]]
}
# last one:
varText <- "r[[2]]"
newDF[,names(eval(parse(text = varText)))] <- eval(parse(text = varText))[[1]]

newDF <- newDF[,-(1),drop=FALSE] # delete first column
pred5081007 <- newDF
saveRDS(pred5081007, "~/Google Drive/Stanford/Stanford Y2 Q1/CS 221/CS221FinalProject/predictions_508_1007.rds")
```










